{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "The goal of this version is to match a simple interface with the integrated commoncrawl loader\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "# don't rerun this if you don't have to\n",
    "#assert(False)\n",
    "start_time = time.time()\n",
    "curve = []\n",
    "\n",
    "urls_in = 'urls_to_load_processing_3.json'\n",
    "path_out = 'processing_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "399\n",
      "1\n",
      "399\n",
      "2\n",
      "399\n",
      "3\n",
      "399\n",
      "4\n",
      "399\n",
      "5\n",
      "399\n",
      "6\n",
      "399\n",
      "7\n",
      "398\n",
      "8\n",
      "399\n",
      "9\n",
      "399\n",
      "10\n",
      "399\n",
      "11\n",
      "399\n",
      "12\n",
      "399\n",
      "13\n",
      "399\n",
      "14\n",
      "399\n",
      "15\n",
      "399\n",
      "16\n",
      "399\n",
      "17\n",
      "398\n",
      "18\n",
      "399\n",
      "19\n",
      "399\n",
      "20\n",
      "399\n",
      "21\n",
      "399\n",
      "22\n",
      "398\n",
      "23\n",
      "399\n",
      "24\n",
      "399\n",
      "25\n",
      "398\n",
      "26\n",
      "399\n",
      "27\n",
      "399\n",
      "28\n",
      "399\n",
      "29\n",
      "399\n",
      "30\n",
      "399\n",
      "31\n",
      "399\n",
      "32\n",
      "399\n",
      "33\n",
      "399\n",
      "34\n",
      "399\n",
      "35\n",
      "398\n",
      "36\n",
      "397\n",
      "37\n",
      "399\n",
      "38\n",
      "398\n",
      "39\n",
      "399\n",
      "40\n",
      "398\n",
      "41\n",
      "399\n",
      "42\n",
      "398\n",
      "43\n",
      "399\n",
      "44\n",
      "399\n",
      "45\n",
      "395\n",
      "46\n",
      "398\n",
      "47\n",
      "399\n",
      "48\n",
      "399\n",
      "49\n",
      "397\n",
      "50\n",
      "399\n",
      "51\n",
      "395\n",
      "52\n",
      "397\n",
      "53\n",
      "396\n",
      "54\n",
      "398\n",
      "55\n",
      "398\n",
      "56\n",
      "398\n",
      "57\n",
      "398\n",
      "58\n",
      "399\n",
      "59\n",
      "398\n",
      "60\n",
      "399\n",
      "61\n",
      "398\n",
      "62\n",
      "398\n",
      "63\n",
      "376\n",
      "progress (/1): 0.9986282578875172\n",
      "nonzero fraction: 0.2774725274725275\n",
      "elapsed time: 391.3041183312734 minutes\n",
      "expected 391.8416239883218 minutes\n",
      "estimated total mins: 2482866.677303637\n",
      "estimated mins remaining: 3405.8527809376665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total = 0\n",
    "combined_dict = {}\n",
    "for i in range(64):\n",
    "    print(i)\n",
    "    path = path_out + 'slice_{}_urls_out.json'.format(i)\n",
    "    path_backup = path_out + 'slice_{}_urls_out_backup.json'.format(i)\n",
    "    \n",
    "    \n",
    "    for i in range(20):\n",
    "        try:\n",
    "            with open(path, encoding='utf8') as f:\n",
    "                out_dict = json.load(f)\n",
    "            \n",
    "            break\n",
    "        except:\n",
    "            try:\n",
    "                with open(path_backup, encoding='utf8') as f:\n",
    "                    out_dict = json.load(f)\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print('fail')\n",
    "            time.sleep(0.05)\n",
    "            out_dict = {}\n",
    "    print(len(out_dict.keys()))\n",
    "    total += len(out_dict.keys())\n",
    "    combined_dict.update(out_dict)\n",
    "\n",
    "with open(urls_in, encoding='utf8') as f:\n",
    "    \n",
    "    urls_to_load = json.load(f)\n",
    "    \n",
    "    \n",
    "progress = total/len(urls_to_load)\n",
    "print('progress (/1): {}'.format(progress))\n",
    "\n",
    "print('nonzero fraction: {}'.format(len([url for url in combined_dict if len(combined_dict[url]) >0])/len(combined_dict)  ))\n",
    "\n",
    "dtime = time.time() - start_time\n",
    "print('elapsed time: {} minutes'.format((dtime)/60 ))\n",
    "\n",
    "\n",
    "print('expected {} minutes'.format(dtime/(total/len(urls_to_load))/60.   ))\n",
    "\n",
    "\n",
    "curve += [(dtime, progress)]\n",
    "\n",
    "\n",
    "\n",
    "## slope in fraction/s\n",
    "slope = (curve[-1][1] - curve[0][1])/(curve[-1][0] - curve[0][0])\n",
    "\n",
    "\n",
    "# estimated total minutes\n",
    "print('estimated total mins: {}'.format(1/slope/60))\n",
    "\n",
    "print('estimated mins remaining: {}'.format((1 - progress)/slope/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_out + 'all.json','w') as f:\n",
    "    f.write(json.dumps(combined_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for processing_3, the first nonzero fraction was 0.18750981315748155\n",
    "This was with a progress of 99.8, but no reruns on empty outputs\n",
    "\n",
    "Retrying now, to rewrite the empty ones!\n",
    "\n",
    "After just a few seconds, got a nonzero fraction of 0.1878630868268174 \n",
    "so definitely rerunning empty ones can help\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum([len(d) for d in dicts_out.values()])\n",
    "\n",
    "combined_html = {}\n",
    "\n",
    "for d in dicts_out.values():\n",
    "    combined_html.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22804"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('urls_to_load.json', encoding='utf8') as f:\n",
    "    \n",
    "    urls_to_load = json.load(f)\n",
    "len(urls_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22532"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_html.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get any urls we missed\n",
    "'''\n",
    "\n",
    "urls_left = {}\n",
    "for key in urls_to_load.keys():\n",
    "    if key not in combined_html.keys():\n",
    "        urls_left[key] = urls_to_load[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is combining the html from the initial run...\n",
    "assert(False)\n",
    "len(urls_left)\n",
    "with open('combined_html_1.json', 'w') as f:\n",
    "    json.dump(combined_html, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping the final urls to grab\n",
    "#\n",
    "# now I've copied the directory, and will reuse the \n",
    "# same commoncrawl code to process the urls that were left over! \n",
    "#\n",
    "# this is in a new directory called commoncrawl_urls_left\n",
    "# once this is done, I will combine it all together\n",
    "\n",
    "with open('urls_left.json', 'w') as f:\n",
    "    json.dump(urls_left, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Finally, combine all the results together\n",
    "# \n",
    "#\n",
    "\n",
    "final_out = {}\n",
    "\n",
    "final_out.update(combined_html)\n",
    "\n",
    "for i in range(16):\n",
    "    print(i)\n",
    "    path = '/homes/gws/pawest/commoncrawl_urls_left/slice_{}_urls_out.json'.format(i)\n",
    "    \n",
    "    with open(path, encoding='utf8') as f:\n",
    "        out_dict = json.load(f)\n",
    "        \n",
    "        final_out.update(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Combine the first run with the 'clean-up' run meant to capture\n",
    "most missed urls (in the end, there were only 9 not caught in either, out of 22000)\n",
    "\n",
    "'''\n",
    "\n",
    "with open('final_html_out.json','w') as f:\n",
    "    json.dump(final_out,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
